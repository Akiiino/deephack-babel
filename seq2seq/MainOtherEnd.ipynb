{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "from torchtext.data import Field, BucketIterator\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext import datasets, data\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import random\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "import math\n",
    "import argparse\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils import clip_grad_norm\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_TOKEN = '<sos>' # XXX : it must be in vocab!\n",
    "EOS_TOKEN = '<eos>'\n",
    "PAD_INDEX = 13\n",
    "\n",
    "MAX_SENS_LENGTH = 20\n",
    "\n",
    "def load_dataset_old(batch_size, filename):\n",
    "    spacy_en = spacy.load('en')\n",
    "    url = re.compile('(<url>.*</url>)')\n",
    "\n",
    "    def tokenize_en(text):\n",
    "        return [tok.text for tok in spacy_en.tokenizer(url.sub('@URL@', text))]\n",
    "\n",
    "    EN = Field(tokenize=tokenize_en, include_lengths=False, init_token=SOS_TOKEN, eos_token=EOS_TOKEN)\n",
    "    \n",
    "    train = datasets.TranslationDataset('./', exts=(filename, filename), fields=(EN, EN))\n",
    "    EN.build_vocab(train.trg, max_size=10000)\n",
    "    \n",
    "    train_iter, test_iter  = BucketIterator.splits((train, train), batch_size=batch_size, repeat=False)\n",
    "    return train_iter\n",
    "\n",
    "def load_dataset(batch_size, filename):\n",
    "    spacy_en = spacy.load('en')\n",
    "    url = re.compile('(<url>.*</url>)')\n",
    "\n",
    "    def tokenize_en(text):\n",
    "        return [tok.text for tok in spacy_en.tokenizer(url.sub('@URL@', text))]\n",
    "\n",
    "    with open(filename, 'rt') as input_file:\n",
    "        inputs = input_file.readlines()\n",
    "    \n",
    "    inputs = [[SOS_TOKEN] +  tokenize_en(_) + [EOS_TOKEN] for _ in inputs]\n",
    "    inputs = [_ for _ in inputs if len(_) < MAX_SENS_LENGTH]\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embed_size, hidden_size, embeddings = None, n_layers = 1, dropout = 0.5):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embed_size = embed_size\n",
    "        self.embed = nn.Embedding(input_size, embed_size)\n",
    "        if embeddings is not None:\n",
    "            self.embed.weight.data = torch.Tensor(embeddings)#.cuda() # TODO : need cuda here?\n",
    "            \n",
    "        # preparation for freeze\n",
    "        # self.embedding.weight.requires_grad = False\n",
    "            \n",
    "        self.gru = nn.GRU(embed_size, hidden_size, n_layers, dropout = dropout, bidirectional = True)\n",
    "\n",
    "    def forward(self, src, hidden=None):\n",
    "\n",
    "        embedded = self.embed(src)\n",
    "        outputs, hidden = self.gru(embedded, hidden)\n",
    "        # sum bidirectional outputs\n",
    "        outputs = (outputs[:, :, :self.hidden_size] +\n",
    "                   outputs[:, :, self.hidden_size:])\n",
    "        return outputs, hidden\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
    "        stdv = 1. / math.sqrt(self.v.size(0))\n",
    "        self.v.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        timestep = encoder_outputs.size(0)\n",
    "        h = hidden.repeat(timestep, 1, 1).transpose(0, 1)\n",
    "        encoder_outputs = encoder_outputs.transpose(0, 1)  # [B*T*H]\n",
    "        attn_energies = self.score(h, encoder_outputs)\n",
    "        return F.softmax(attn_energies, dim=1).unsqueeze(1)\n",
    "\n",
    "    def score(self, hidden, encoder_outputs):\n",
    "        # [B*T*2H]->[B*T*H]\n",
    "        energy = self.attn(torch.cat([hidden, encoder_outputs], 2))\n",
    "        energy = energy.transpose(1, 2)  # [B*H*T]\n",
    "        v = self.v.repeat(encoder_outputs.size(0), 1).unsqueeze(1)  # [B*1*H]\n",
    "        energy = torch.bmm(v, energy)  # [B*1*T]\n",
    "        return energy.squeeze(1)  # [B*T]\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, output_size, embeddings = None, n_layers = 1, dropout = 0.2):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embed = nn.Embedding(output_size, embed_size)\n",
    "        \n",
    "        if embeddings is not None:\n",
    "            self.embed.weight.data = torch.Tensor(embeddings)#.cuda() # TODO : need cuda here?\n",
    "            \n",
    "        # preparation for freeze\n",
    "        # self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout, inplace=True)\n",
    "        self.attention = Attention(hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size + embed_size, hidden_size, n_layers, dropout=dropout)\n",
    "        self.out = nn.Linear(hidden_size * 2, output_size)\n",
    "\n",
    "    def forward(self, input, last_hidden, encoder_outputs):\n",
    "        # Get the embedding of the current input word (last output word)\n",
    "        embedded = self.embed(input).unsqueeze(0)  # (1,B,N)\n",
    "        embedded = self.dropout(embedded)\n",
    "        # Calculate attention weights and apply to encoder outputs\n",
    "        attn_weights = self.attention(last_hidden[-1], encoder_outputs)\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))  # (B,1,N)\n",
    "        context = context.transpose(0, 1)  # (1,B,N)\n",
    "        # Combine embedded input word and attended context, run through RNN\n",
    "        rnn_input = torch.cat([embedded, context], 2)\n",
    "        output, hidden = self.gru(rnn_input, last_hidden)\n",
    "        output = output.squeeze(0)  # (1,B,N) -> (B,N)\n",
    "        context = context.squeeze(0)\n",
    "        output = self.out(torch.cat([output, context], 1))\n",
    "        output = F.log_softmax(output, dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        batch_size = src.size(1)\n",
    "        max_len = trg.size(0)\n",
    "        vocab_size = self.decoder.output_size\n",
    "        outputs = Variable(torch.zeros(max_len, batch_size, vocab_size)).cuda()\n",
    "\n",
    "        encoder_output, hidden = self.encoder(src)\n",
    "        hidden = hidden[:self.decoder.n_layers]\n",
    "        output = Variable(trg.data[0, :])  # sos\n",
    "        for t in range(1, max_len):\n",
    "            output, hidden, attn_weights = self.decoder(\n",
    "                    output, hidden, encoder_output)\n",
    "            outputs[t] = output\n",
    "            is_teacher = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.data.max(1)[1] # TODO : beam search here\n",
    "            output = Variable(trg.data[t] if is_teacher else top1).cuda()\n",
    "        return outputs\n",
    "\n",
    "    \n",
    "class DoubleTranslator(nn.Module):\n",
    "    def __init__(self, common_encoder, first_lang_decoder, second_lang_decoder):\n",
    "        super(DoubleTranslator, self).__init__()\n",
    "        self.common_encoder = common_encoder\n",
    "        self.first_lang_decoder = first_lang_decoder\n",
    "        self.second_lang_decoder = second_lang_decoder\n",
    "        self.is_from_first_lang_to_second = True\n",
    "        \n",
    "    def set_is_from_first_lang_to_second(self, value):\n",
    "        self.is_from_first_lang_to_second = value\n",
    "\n",
    "    def get_is_from_first_lang_to_second(self):\n",
    "        return self.is_from_first_lang_to_second\n",
    "    \n",
    "    def forward_one_lang(self, src, trg, teacher_forcing_ratio=0.5, is_first_lang = True):\n",
    "        batch_size = src.size(1)\n",
    "        max_len = trg.size(0)\n",
    "\n",
    "        # if is_first_lang: en_word -> encoder -> ru_decoder -> encoder -> en_decoder -> en_word\n",
    "        decoder = self.first_lang_decoder if not is_first_lang else self.second_lang_decoder\n",
    "        \n",
    "        vocab_size = decoder.output_size\n",
    "        outputs = Variable(torch.zeros(max_len, batch_size, vocab_size)).cuda()\n",
    "        \n",
    "        encoder_output, hidden = self.common_encoder(src)\n",
    "        hidden = hidden[:decoder.n_layers]\n",
    "        output = Variable(trg.data[0, :])  # sos\n",
    "        for t in range(1, max_len):\n",
    "            output, hidden, attn_weights = decoder(output, hidden, encoder_output)\n",
    "            outputs[t] = output\n",
    "            is_teacher = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.data.max(1)[1] # TODO : beam search here\n",
    "            output = Variable(trg.data[t] if is_teacher else top1).cuda()\n",
    "            \n",
    "        return outputs\n",
    "    \n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        output_first_lang = self.forward_one_lang(src, trg, teacher_forcing_ratio, self.get_is_from_first_lang_to_second())\n",
    "        return self.forward_one_lang(src, trg, teacher_forcing_ratio, not self.get_is_from_first_lang_to_second())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start create ru vocab and embs\n",
      "finish create ru vocab and embs\n",
      "[!] preparing dataset...\n",
      "[ru_vocab]:44326 [en_vocab]:400000\n",
      "[!] Instantiating models...\n",
      "[100][loss:12.01][pp:164751.42]\n",
      "[200][loss: 7.28][pp:1456.13]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-611ecf60e53d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[0;31m#         train(e, second_lang_seq2seq, second_lang_optimizer, second_lang_train_iter, ru_size, grad_clip, ru_vocab, batch_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m         train_trans(e, double_translator, decoder_optimizer, first_lang_train_iter,\n\u001b[0;32m--> 201\u001b[0;31m                     en_size, grad_clip, en_vocab, batch_size)\n\u001b[0m\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m     \u001b[0;31m# TODO: use val_iter here\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-611ecf60e53d>\u001b[0m in \u001b[0;36mtrain_trans\u001b[0;34m(e, model, optimizer, train_iter, vocab_size, grad_clip, en_vocab, batch_size)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         loss = F.cross_entropy(output[1:].view(-1, vocab_size),\n\u001b[1;32m     85\u001b[0m                                \u001b[0mtrg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-7e7cc0713c94>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, trg, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteacher_forcing_ratio\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0moutput_first_lang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_one_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteacher_forcing_ratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_is_from_first_lang_to_second\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_one_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mteacher_forcing_ratio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_is_from_first_lang_to_second\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-7e7cc0713c94>\u001b[0m in \u001b[0;36mforward_one_lang\u001b[0;34m(self, src, trg, teacher_forcing_ratio, is_first_lang)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mvocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mencoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcommon_encoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def batch(iterable, batch_size = 1):\n",
    "    all_length = len(iterable)\n",
    "    for ndx in range(0, all_length, batch_size):\n",
    "        yield iterable[ndx:min(ndx + batch_size, all_length)]\n",
    "\n",
    "def get_vocab(emb_plain):\n",
    "    result = {}\n",
    "    for i, line in enumerate(en_emb_plain):\n",
    "        word, vector = line.split(' ', 1)\n",
    "        result[word] = len(result)  \n",
    "\n",
    "    return result\n",
    "\n",
    "def get_embeddings(emb_plain, emb_size):\n",
    "    result = np.ndarray((len(emb_plain), emb_size), dtype='float32')\n",
    "    for i, line in enumerate(en_emb_plain):\n",
    "        word, vector = line.split(' ', 1)\n",
    "        result[i] = vector.split()  \n",
    "    return result\n",
    "        \n",
    "def evaluate(model, val_iter, vocab_size, en_vocab):\n",
    "    model.eval()\n",
    "    pad = PAD_INDEX\n",
    "    total_loss = 0\n",
    "    for b, batch in enumerate(val_iter):\n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "        src = Variable(src.data.cuda(), volatile=True)\n",
    "        trg = Variable(trg.data.cuda(), volatile=True)\n",
    "        output = model(src, trg)\n",
    "        loss = F.cross_entropy(output[1:].view(-1, vocab_size),\n",
    "                               trg[1:].contiguous().view(-1),\n",
    "                               ignore_index=pad)\n",
    "        total_loss += loss.data[0]\n",
    "    return total_loss / len(val_iter)\n",
    "\n",
    "\n",
    "def train(e, model, optimizer, train_iter, vocab_size, grad_clip, en_vocab, batch_size):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    pad = PAD_INDEX\n",
    "    \n",
    "    for b, sents in enumerate(batch(train_iter, batch_size=batch_size)):\n",
    "        max_length_in_batch = max([len(s) for s in sents])\n",
    "        \n",
    "        t = np.array([sents2inds(sent, max_length_in_batch, en_vocab) for sent in sents])\n",
    "        src = Variable(torch.from_numpy(t.T))\n",
    "        trg = Variable(torch.from_numpy(t.T))\n",
    "        \n",
    "        src, trg = src.cuda(), trg.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg)\n",
    "        loss = F.cross_entropy(output[1:].view(-1, vocab_size),\n",
    "                               trg[1:].contiguous().view(-1),\n",
    "                               ignore_index=pad)\n",
    "        loss.backward()\n",
    "        clip_grad_norm(model.parameters(), grad_clip)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.data[0]\n",
    "\n",
    "        if b % 100 == 0 and b != 0:\n",
    "            total_loss = total_loss / 100\n",
    "            print(\"[%d][loss:%5.2f][pp:%5.2f]\" %\n",
    "                  (b, total_loss, math.exp(total_loss)))\n",
    "            total_loss = 0\n",
    "\n",
    "\n",
    "def train_trans(e, model, optimizer, train_iter,\n",
    "                    vocab_size, grad_clip, en_vocab, batch_size):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    pad = PAD_INDEX\n",
    "    \n",
    "    for b, sents in enumerate(batch(train_iter, batch_size=batch_size)):\n",
    "        max_length_in_batch = max([len(s) for s in sents])\n",
    "        \n",
    "        t = np.array([sents2inds(sent, max_length_in_batch, en_vocab) for sent in sents])\n",
    "        src = Variable(torch.from_numpy(t.T))\n",
    "        trg = Variable(torch.from_numpy(t.T))\n",
    "        \n",
    "        src, trg = src.cuda(), trg.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg)\n",
    "        loss = F.cross_entropy(output[1:].view(-1, vocab_size),\n",
    "                               trg[1:].contiguous().view(-1),\n",
    "                               ignore_index=pad)\n",
    "        loss.backward()\n",
    "        clip_grad_norm(model.parameters(), grad_clip)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.data[0]\n",
    "\n",
    "        if b % 100 == 0 and b != 0:\n",
    "            total_loss = total_loss / 100\n",
    "            print(\"[%d][loss:%5.2f][pp:%5.2f]\" %\n",
    "                  (b, total_loss, math.exp(total_loss)))\n",
    "            total_loss = 0\n",
    "\n",
    "            \n",
    "def create_ind2word(vocab):\n",
    "    return {item[1]:item[0] for item in vocab.items()}\n",
    "            \n",
    "def create_emb_and_dict_by_emb_and_dict(vocab, embeddings, ind2word, dict_plain):\n",
    "    result_emb = []\n",
    "    result_dict = {}\n",
    "    \n",
    "    # XXX : first 3 items just copy : SOS EOS ...\n",
    "    count = 3\n",
    "    for i in range(count):\n",
    "        en_word = ind2word.get(i, -1)\n",
    "        if en_word == -1:\n",
    "            continue\n",
    "            \n",
    "        result_emb.append(embeddings[i])\n",
    "        result_dict[en_word] = len(result_dict)\n",
    "    \n",
    "    for word_pair in dict_plain:\n",
    "        en_word, ru_word = word_pair.split()\n",
    "        \n",
    "        if en_word not in vocab or ru_word in result_dict:\n",
    "            continue\n",
    "        \n",
    "        result_emb.append(embeddings[vocab[en_word]])\n",
    "        result_dict[ru_word] = len(result_dict)\n",
    "        \n",
    "    return result_dict, result_emb\n",
    "    \n",
    "\n",
    "def sents2inds(sents, max_length, vocab):\n",
    "    pad_ind = PAD_INDEX\n",
    "    return np.pad(np.array([en_vocab.get(i, 1) for i in sents]),\n",
    "                (0, max_length - len(sents)),  mode='constant', constant_values=(pad_ind))\n",
    "    \n",
    "# def main():\n",
    "if True:    \n",
    "    with open('glove/glove.6B.50d.txt', 'rt') as emb_file:\n",
    "        en_emb_plain = emb_file.readlines()\n",
    "\n",
    "    with open('en-ru.txt', 'rt') as dict_file:\n",
    "        dict_en_ru_plain = dict_file.readlines()\n",
    "        \n",
    "    en_vocab = get_vocab(en_emb_plain)\n",
    "    en_embeddings = get_embeddings(en_emb_plain, 50)\n",
    "    en_ind2word = create_ind2word(en_vocab)\n",
    "    \n",
    "    print('start create ru vocab and embs')\n",
    "    ru_vocab, ru_embeddings = create_emb_and_dict_by_emb_and_dict(en_vocab, en_embeddings, en_ind2word, dict_en_ru_plain)\n",
    "    print('finish create ru vocab and embs')\n",
    "    \n",
    "    epochs = 10\n",
    "    grad_clip = 10.0\n",
    "    lr = 0.0001\n",
    "    en_size = len(en_emb_plain)\n",
    "    \n",
    "    batch_size = 5\n",
    "    hidden_size = 64\n",
    "    embed_size = 50\n",
    "    assert torch.cuda.is_available()\n",
    "\n",
    "    print(\"[!] preparing dataset...\")\n",
    "    \n",
    "    first_lang_train_iter = load_dataset(batch_size, 'corpus1_10k.txt')\n",
    "    second_lang_train_iter = load_dataset(batch_size, 'corpus2_10k.txt')\n",
    "            \n",
    "    en_size = len(en_vocab)\n",
    "    ru_size = len(ru_vocab)\n",
    "\n",
    "    \n",
    "#     de_size, en_size = len(DE.vocab), len(EN.vocab)\n",
    "#     print(\"[TRAIN]:%d (dataset:%d)\\t[TEST]:%d (dataset:%d)\"\n",
    "#           % (len(train_iter), len(train_iter.dataset),\n",
    "#              len(test_iter), len(test_iter.dataset)))\n",
    "    \n",
    "    de_size = en_size\n",
    "    print(\"[ru_vocab]:%d [en_vocab]:%d\" % (ru_size, en_size))\n",
    "\n",
    "    print(\"[!] Instantiating models...\")\n",
    "   \n",
    "\n",
    "    en_embeddings = get_embeddings(en_emb_plain, 50)\n",
    "    common_encoder = Encoder(en_size, embed_size, hidden_size, en_embeddings, n_layers=2, dropout=0.5)\n",
    "    first_lang_decoder = Decoder(embed_size, hidden_size, en_size, en_embeddings, n_layers=1, dropout=0.5)\n",
    "    second_lang_decoder = Decoder(embed_size, hidden_size, en_size, en_embeddings, n_layers=1, dropout=0.5)\n",
    "\n",
    "    first_lang_seq2seq = Seq2Seq(common_encoder, first_lang_decoder).cuda()\n",
    "    second_lang_seq2seq = Seq2Seq(common_encoder, second_lang_decoder).cuda()\n",
    "    double_translator = DoubleTranslator(common_encoder, first_lang_decoder, second_lang_decoder).cuda()\n",
    "\n",
    "    first_lang_optimizer = optim.Adam(first_lang_seq2seq.parameters(), lr=lr)\n",
    "    second_lang_optimizer = optim.Adam(second_lang_seq2seq.parameters(), lr=lr)\n",
    "    decoder_optimizer = optim.Adam(double_translator.parameters(), lr=lr)\n",
    "\n",
    "#     print(first_lang_seq2seq)\n",
    "#     print(second_lang_seq2seq)\n",
    "#     print(double_translator)\n",
    "\n",
    "    best_val_loss = None\n",
    "    for e in range(1, epochs+1):\n",
    "#         train(e, first_lang_seq2seq, first_lang_optimizer, first_lang_train_iter, en_size, grad_clip, en_vocab, batch_size)\n",
    "#         train(e, second_lang_seq2seq, second_lang_optimizer, second_lang_train_iter, ru_size, grad_clip, ru_vocab, batch_size)\n",
    "        train_trans(e, double_translator, decoder_optimizer, first_lang_train_iter,\n",
    "                    en_size, grad_clip, en_vocab, batch_size)\n",
    "        \n",
    "    # TODO: use val_iter here\n",
    "#         first_lang_val_loss = evaluate(first_lang_seq2seq, shuffled_train_iter, en_size, en_vocab)\n",
    "#         second_lang_val_loss = evaluate(second_lang_seq2seq, val_iter, en_size, DE, DE)\n",
    "#         double_trans_val_loss = evaluate(double_translator, val_iter, en_size, DE, DE)\n",
    "\n",
    "#         val_loss = first_lang_val_loss + second_lang_val_loss + double_trans_val_loss\n",
    "        \n",
    "        print(\"[Epoch:%d] val_loss:%5.3f | val_pp:%5.2fS\" % (e, val_loss, math.exp(val_loss)))\n",
    "\n",
    "        # Save the model if the validation loss is the best we've seen so far.\n",
    "        if not best_val_loss or val_loss < best_val_loss:\n",
    "            print(\"[!] saving model...\")\n",
    "            if not os.path.isdir(\".save\"):\n",
    "                os.makedirs(\".save\")\n",
    "            torch.save(first_lang_seq2seq.state_dict(), './.save/seq2seq_%d.pt' % (e))\n",
    "            best_val_loss = val_loss\n",
    "    test_loss = evaluate(first_lang_seq2seq, test_iter, en_size)\n",
    "    print(\"[TEST] loss:%5.2f\" % test_loss)\n",
    "\n",
    "\n",
    "\n",
    "# main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
