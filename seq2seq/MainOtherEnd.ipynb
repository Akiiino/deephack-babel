{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "from torchtext.data import Field, BucketIterator\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext import datasets, data\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import random\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "import math\n",
    "import argparse\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils import clip_grad_norm\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_TOKEN = '<sos>' # XXX : it must be in vocab!\n",
    "EOS_TOKEN = '<eos>'\n",
    "PAD_INDEX = 13 # TODO\n",
    "UNK_INDEX = 1 # TODO\n",
    "\n",
    "MAX_SENS_LENGTH = 20\n",
    "\n",
    "def load_dataset_old(batch_size, filename):\n",
    "    spacy_en = spacy.load('en')\n",
    "    url = re.compile('(<url>.*</url>)')\n",
    "\n",
    "    def tokenize_en(text):\n",
    "        return [tok.text for tok in spacy_en.tokenizer(url.sub('@URL@', text))]\n",
    "\n",
    "    EN = Field(tokenize=tokenize_en, include_lengths=False, init_token=SOS_TOKEN, eos_token=EOS_TOKEN)\n",
    "    \n",
    "    train = datasets.TranslationDataset('./', exts=(filename, filename), fields=(EN, EN))\n",
    "    EN.build_vocab(train.trg, max_size=10000)\n",
    "    \n",
    "    train_iter, test_iter  = BucketIterator.splits((train, train), batch_size=batch_size, repeat=False)\n",
    "    return train_iter\n",
    "\n",
    "def load_dataset(batch_size, filename, val_ratio=0.2):\n",
    "    spacy_en = spacy.load('en')\n",
    "    url = re.compile('(<url>.*</url>)')\n",
    "\n",
    "    def tokenize_en(text):\n",
    "        return [tok.text for tok in spacy_en.tokenizer(url.sub('@URL@', text))]\n",
    "\n",
    "    with open(filename, 'rt') as input_file:\n",
    "        inputs = input_file.readlines()\n",
    "    \n",
    "    inputs = [[SOS_TOKEN] +  tokenize_en(_) + [EOS_TOKEN] for _ in inputs]\n",
    "    inputs = [_ for _ in inputs if len(_) < MAX_SENS_LENGTH]\n",
    "    np.random.shuffle(inputs)\n",
    "    \n",
    "    idx_train = int(len(inputs)*(1-val_ratio))\n",
    "    inputs_train = inputs[:idx_train]\n",
    "    inputs_val = inputs[idx_train:] \n",
    "        \n",
    "    return inputs_train, inputs_val\n",
    "\n",
    "\n",
    "def load_parallel_dataset(batch_size, filename1, filename2):\n",
    "    spacy_en = spacy.load('en')\n",
    "    url = re.compile('(<url>.*</url>)')\n",
    "\n",
    "    def tokenize_en(text):\n",
    "        return [tok.text for tok in spacy_en.tokenizer(url.sub('@URL@', text))]\n",
    "\n",
    "    with open(filename1, 'rt') as input_file:\n",
    "        inputs1 = input_file.readlines()\n",
    "    \n",
    "    with open(filename2, 'rt') as input_file:\n",
    "        inputs2 = input_file.readlines()\n",
    "    \n",
    "    inputs1 = [[SOS_TOKEN] +  tokenize_en(_) + [EOS_TOKEN] for _ in inputs1]\n",
    "    inputs2 = [[SOS_TOKEN] +  tokenize_en(_) + [EOS_TOKEN] for _ in inputs2]\n",
    "\n",
    "    inputs1 = [_ for _ in inputs1 if len(_) < MAX_SENS_LENGTH]\n",
    "    inputs2 = [_ for _ in inputs2 if len(_) < MAX_SENS_LENGTH]\n",
    "    \n",
    "    return inputs1, inputs2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embed_size, hidden_size, embeddings = None, n_layers = 1, dropout = 0.5):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embed_size = embed_size\n",
    "        self.embed = nn.Embedding(input_size, embed_size)\n",
    "        if embeddings is not None:\n",
    "            self.embed.weight.data = torch.Tensor(embeddings)#.cuda() # TODO : need cuda here?\n",
    "            \n",
    "        # preparation for freeze\n",
    "        self.embed.weight.requires_grad = False\n",
    "            \n",
    "        self.gru = nn.GRU(embed_size, hidden_size, n_layers, dropout = dropout, bidirectional = True)\n",
    "\n",
    "    def forward(self, src, hidden=None):\n",
    "\n",
    "        embedded = self.embed(src)\n",
    "        outputs, hidden = self.gru(embedded, hidden)\n",
    "        # sum bidirectional outputs\n",
    "        outputs = (outputs[:, :, :self.hidden_size] +\n",
    "                   outputs[:, :, self.hidden_size:])\n",
    "        return outputs, hidden\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
    "        stdv = 1. / math.sqrt(self.v.size(0))\n",
    "        self.v.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        timestep = encoder_outputs.size(0)\n",
    "        h = hidden.repeat(timestep, 1, 1).transpose(0, 1)\n",
    "        encoder_outputs = encoder_outputs.transpose(0, 1)  # [B*T*H]\n",
    "        attn_energies = self.score(h, encoder_outputs)\n",
    "        return F.softmax(attn_energies, dim=1).unsqueeze(1)\n",
    "\n",
    "    def score(self, hidden, encoder_outputs):\n",
    "        # [B*T*2H]->[B*T*H]\n",
    "        energy = self.attn(torch.cat([hidden, encoder_outputs], 2))\n",
    "        energy = energy.transpose(1, 2)  # [B*H*T]\n",
    "        v = self.v.repeat(encoder_outputs.size(0), 1).unsqueeze(1)  # [B*1*H]\n",
    "        energy = torch.bmm(v, energy)  # [B*1*T]\n",
    "        return energy.squeeze(1)  # [B*T]\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, output_size, embeddings = None, n_layers = 1, dropout = 0.2):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embed = nn.Embedding(output_size, embed_size)\n",
    "        \n",
    "        if embeddings is not None:\n",
    "            self.embed.weight.data = torch.Tensor(embeddings)#.cuda() # TODO : need cuda here?\n",
    "            \n",
    "        # preparation for freeze\n",
    "        self.embed.weight.requires_grad = False\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout, inplace=True)\n",
    "        self.attention = Attention(hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size + embed_size, hidden_size, n_layers, dropout=dropout)\n",
    "        self.out = nn.Linear(hidden_size * 2, output_size)\n",
    "\n",
    "    def forward(self, input, last_hidden, encoder_outputs):\n",
    "        # Get the embedding of the current input word (last output word)\n",
    "        embedded = self.embed(input).unsqueeze(0)  # (1,B,N)\n",
    "        embedded = self.dropout(embedded)\n",
    "        # Calculate attention weights and apply to encoder outputs\n",
    "        attn_weights = self.attention(last_hidden[-1], encoder_outputs)\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))  # (B,1,N)\n",
    "        context = context.transpose(0, 1)  # (1,B,N)\n",
    "        # Combine embedded input word and attended context, run through RNN\n",
    "        rnn_input = torch.cat([embedded, context], 2)\n",
    "        output, hidden = self.gru(rnn_input, last_hidden)\n",
    "        output = output.squeeze(0)  # (1,B,N) -> (B,N)\n",
    "        context = context.squeeze(0)\n",
    "        output = self.out(torch.cat([output, context], 1))\n",
    "        output = F.log_softmax(output, dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        batch_size = src.size(1)\n",
    "        max_len = trg.size(0)\n",
    "        vocab_size = self.decoder.output_size\n",
    "        outputs = Variable(torch.zeros(max_len, batch_size, vocab_size)).cuda()\n",
    "\n",
    "        encoder_output, hidden = self.encoder(src)\n",
    "        hidden = hidden[:self.decoder.n_layers]\n",
    "        output = Variable(trg.data[0, :])  # sos\n",
    "        for t in range(1, max_len):\n",
    "            output, hidden, attn_weights = self.decoder(\n",
    "                    output, hidden, encoder_output)\n",
    "            outputs[t] = output\n",
    "            is_teacher = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.data.max(1)[1] # TODO : beam search here\n",
    "            output = Variable(trg.data[t] if is_teacher else top1).cuda()\n",
    "        return outputs\n",
    "\n",
    "    \n",
    "class DoubleTranslator(nn.Module):\n",
    "    def __init__(self, common_encoder, first_lang_decoder, second_lang_decoder):\n",
    "        super(DoubleTranslator, self).__init__()\n",
    "        self.common_encoder = common_encoder\n",
    "        self.first_lang_decoder = first_lang_decoder\n",
    "        self.second_lang_decoder = second_lang_decoder\n",
    "        self.is_from_first_lang_to_second = True\n",
    "        \n",
    "    def set_is_from_first_lang_to_second(self, value):\n",
    "        self.is_from_first_lang_to_second = value\n",
    "\n",
    "    def get_is_from_first_lang_to_second(self):\n",
    "        return self.is_from_first_lang_to_second\n",
    "    \n",
    "    def forward_one_lang(self, src, trg, teacher_forcing_ratio=0.5, is_first_lang = True):\n",
    "        batch_size = src.size(1)\n",
    "        max_len = trg.size(0)\n",
    "\n",
    "        # if is_first_lang: en_word -> encoder -> ru_decoder -> encoder -> en_decoder -> en_word\n",
    "        decoder = self.first_lang_decoder if not is_first_lang else self.second_lang_decoder\n",
    "        \n",
    "        vocab_size = decoder.output_size\n",
    "        outputs = Variable(torch.zeros(max_len, batch_size, vocab_size)).cuda()\n",
    "        \n",
    "        encoder_output, hidden = self.common_encoder(src)\n",
    "        hidden = hidden[:decoder.n_layers]\n",
    "        output = Variable(trg.data[0, :])  # sos\n",
    "        for t in range(1, max_len):\n",
    "            output, hidden, attn_weights = decoder(output, hidden, encoder_output)\n",
    "            outputs[t] = output\n",
    "            is_teacher = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.data.max(1)[1] # TODO : beam search here\n",
    "            output = Variable(trg.data[t] if is_teacher else top1).cuda()\n",
    "            \n",
    "        return outputs\n",
    "    \n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        output_first_lang = self.forward_one_lang(src, trg, teacher_forcing_ratio, self.get_is_from_first_lang_to_second())\n",
    "        return self.forward_one_lang(src, trg, teacher_forcing_ratio, not self.get_is_from_first_lang_to_second())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ru_vocab]:44323 [en_vocab]:41976 [common_size]:41976\n",
      "[!] preparing dataset...\n",
      "[!] Instantiating models...\n",
      "41976\n",
      "start train two auto-encoders with swap perc  0.02727272727272727\n",
      "[100][loss:16.78][pp:19445296.96]\n"
     ]
    }
   ],
   "source": [
    "def batch(iterable, batch_size = 1):\n",
    "    all_length = len(iterable)\n",
    "    for ndx in range(0, all_length, batch_size):\n",
    "        yield iterable[ndx:min(ndx + batch_size, all_length)]\n",
    "\n",
    "def get_vocab(emb_plain):\n",
    "    result = {}\n",
    "    for i, line in enumerate(emb_plain):\n",
    "        word, vector = line.split(' ', 1)\n",
    "        result[word] = len(result)  \n",
    "\n",
    "    return result\n",
    "\n",
    "def get_embeddings(emb_plain, emb_size):\n",
    "    result = np.ndarray((len(emb_plain), emb_size), dtype='float32')\n",
    "    for i, line in enumerate(emb_plain):\n",
    "        word, vector = line.split(' ', 1)\n",
    "        result[i] = vector.split()  \n",
    "    return result\n",
    "        \n",
    "class TrainData:\n",
    "    def __init__(self, model, train_iter, vocab_size, vocab):\n",
    "        self.model = model\n",
    "        self.train_iter = train_iter\n",
    "        self.vocab_size = vocab_size\n",
    "        self.vocab = vocab\n",
    "\n",
    "def train(e, train_data1, train_data2, grad_clip, batch_size, optimizer, proc_shuffle = 0.0, is_train = True):\n",
    "    if is_train:\n",
    "        train_data1.model.train()\n",
    "        train_data2.model.train()\n",
    "    else:\n",
    "        train_data1.model.eval()\n",
    "        train_data2.model.eval()\n",
    "    \n",
    "    total_loss = 0\n",
    "    pad = PAD_INDEX\n",
    "    \n",
    "    common_length = min(len(train_data1.train_iter), len(train_data2.train_iter))\n",
    "    common_data = list(zip(train_data1.train_iter[:common_length], train_data2.train_iter[:common_length]))\n",
    "        \n",
    "    def permute(arr, percent_of_swaps = 0.5):\n",
    "        assert percent_of_swaps < 1\n",
    "        count_of_swaps = (int)(percent_of_swaps * len(arr))\n",
    "\n",
    "        indeces = np.random.randint(arr.shape[0], size=(count_of_swaps, 2))\n",
    "\n",
    "        ans = arr.copy()\n",
    "        ans[indeces[:, 0]], ans[indeces[:, 1]] = ans[indeces[:, 1]], ans[indeces[:, 0]]\n",
    "        return ans\n",
    "    \n",
    "    def get_loss(sents, train_data1, proc_shuffle):\n",
    "        max_length_in_batch1 = max([len(s) for s in sents1])\n",
    "        \n",
    "        t1 = np.array([permute(sents2inds(sent, max_length_in_batch1, train_data1.vocab), proc_shuffle) for sent in sents1])\n",
    "        src1 = Variable(torch.from_numpy(t1.T))\n",
    "        trg1 = Variable(torch.from_numpy(t1.T))\n",
    "        src1, trg1 = src1.cuda(), trg1.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output1 = train_data1.model(src1, trg1)\n",
    "\n",
    "        loss1 = F.cross_entropy(output1[1:].view(-1, train_data1.vocab_size),\n",
    "                               trg1[1:].contiguous().view(-1),\n",
    "                               ignore_index=pad)\n",
    "        return loss1\n",
    "    \n",
    "    total_loss = 0\n",
    "    global_total_loss = 0\n",
    "\n",
    "\n",
    "    iter_count = 0\n",
    "    for b, sents in enumerate(batch(common_data, batch_size=batch_size)):\n",
    "        iter_count += 1\n",
    "        \n",
    "        sents1 = [s[0] for s in sents]\n",
    "        sents2 = [s[1] for s in sents]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = get_loss(sents1, train_data1, proc_shuffle) + get_loss(sents2, train_data2, proc_shuffle) \n",
    "        loss.backward()\n",
    "        \n",
    "        clip_grad_norm(train_data1.model.parameters(), grad_clip)\n",
    "        clip_grad_norm(train_data2.model.parameters(), grad_clip)\n",
    "        \n",
    "        if is_train:\n",
    "            optimizer.step()\n",
    "        total_loss += loss.data[0]\n",
    "\n",
    "        global_total_loss += loss.data[0]\n",
    "        if b % 100 == 0 and b != 0:\n",
    "            total_loss = total_loss / 100\n",
    "            print(\"[%d][loss:%5.2f][pp:%5.2f]\" %\n",
    "                  (b, total_loss, math.exp(total_loss)))\n",
    "            total_loss = 0\n",
    "    return global_total_loss / iter_count\n",
    "\n",
    "\n",
    "def train_trans(e, model, optimizer, train_iter,\n",
    "                    vocab_size, grad_clip, vocab, batch_size):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    pad = PAD_INDEX\n",
    "    \n",
    "    for b, sents in enumerate(batch(train_iter, batch_size=batch_size)):\n",
    "        max_length_in_batch = max([len(s) for s in sents])\n",
    "        \n",
    "        t = np.array([sents2inds(sent, max_length_in_batch, vocab) for sent in sents])\n",
    "        src = Variable(torch.from_numpy(t.T))\n",
    "        trg = Variable(torch.from_numpy(t.T))\n",
    "        \n",
    "        src, trg = src.cuda(), trg.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg)\n",
    "        loss = F.cross_entropy(output[1:].view(-1, vocab_size),\n",
    "                               trg[1:].contiguous().view(-1),\n",
    "                               ignore_index=pad)\n",
    "        loss.backward()\n",
    "        clip_grad_norm(model.parameters(), grad_clip)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.data[0]\n",
    "\n",
    "        if b % 100 == 0 and b != 0:\n",
    "            total_loss = total_loss / 100\n",
    "            print(\"[%d][loss:%5.2f][pp:%5.2f]\" %\n",
    "                  (b, total_loss, math.exp(total_loss)))\n",
    "            total_loss = 0\n",
    "\n",
    "            \n",
    "def create_ind2word(vocab):\n",
    "    return {item[1]:item[0] for item in vocab.items()}\n",
    "            \n",
    "def create_emb_and_dict_by_emb_and_dict(vocab, embeddings, ind2word, dict_plain):\n",
    "    result_emb = []\n",
    "    result_dict = {}\n",
    "    \n",
    "    result_emb2 = []\n",
    "    result_dict2 = {}\n",
    "    \n",
    "    res_embedding_inds = []\n",
    "    \n",
    "    # XXX : first 3 items just copy : SOS EOS ...\n",
    "#     count = 3\n",
    "#     for i in range(count):\n",
    "#         en_word = ind2word.get(i, -1)\n",
    "#         if en_word == -1:\n",
    "#             continue\n",
    "            \n",
    "#         result_emb.append(embeddings[i])\n",
    "#         result_dict[en_word] = len(result_dict)\n",
    "        \n",
    "#         result_emb2.append(embeddings[i])\n",
    "#         result_dict2[en_word] = len(result_dict)\n",
    "    \n",
    "    for word_pair in dict_plain:\n",
    "        en_word, ru_word = word_pair.split()\n",
    "        \n",
    "        if en_word not in vocab or ru_word in result_emb:\n",
    "            continue\n",
    "        \n",
    "        if en_word in result_dict2:\n",
    "            result_dict[ru_word] = result_dict2[en_word]\n",
    "            continue\n",
    "        \n",
    "        res_embedding_inds.append(vocab[en_word])\n",
    "        result_dict[ru_word] = len(result_dict2)\n",
    "        result_dict2[en_word] = len(result_dict2)\n",
    "        \n",
    "    result_emb = embeddings[res_embedding_inds]\n",
    "    return result_dict, result_emb, result_dict2, result_emb\n",
    "\n",
    "def sents2inds(sents, max_length, vocab):\n",
    "    pad_ind = PAD_INDEX\n",
    "    return np.pad(np.array([vocab.get(i, UNK_INDEX) for i in sents]),\n",
    "                (0, max_length - len(sents)),  mode='constant', constant_values=(pad_ind))\n",
    "    \n",
    "# def main():\n",
    "if True:    \n",
    "    with open('glove/glove.6B.50d.txt', 'rt') as emb_file:\n",
    "        en_emb_plain = emb_file.readlines()\n",
    "\n",
    "    with open('en-ru.txt', 'rt') as dict_file:\n",
    "        dict_en_ru_plain = dict_file.readlines()\n",
    "        \n",
    "    en_vocab = get_vocab(en_emb_plain)\n",
    "    en_embeddings = get_embeddings(en_emb_plain, 50)\n",
    "    en_ind2word = create_ind2word(en_vocab)\n",
    "    \n",
    "    ru_vocab, ru_embeddings, en_vocab, en_embeddings = create_emb_and_dict_by_emb_and_dict(en_vocab, en_embeddings, en_ind2word, dict_en_ru_plain)\n",
    "#     ru_embeddings = en_embeddings = None\n",
    "\n",
    "    en_size = len(en_vocab)\n",
    "    ru_size = len(ru_vocab)\n",
    "    \n",
    "    common_size = min(en_size, ru_size)\n",
    "    \n",
    "    print(\"[ru_vocab]:%d [en_vocab]:%d [common_size]:%d\" % (ru_size, en_size, common_size))\n",
    "\n",
    "    en_size = common_size\n",
    "    ru_size = common_size\n",
    "\n",
    "    \n",
    "    epochs = 10\n",
    "    grad_clip = 10.0\n",
    "    lr = 0.0001\n",
    "#     en_size = len(en_emb_plain)\n",
    "    \n",
    "    batch_size = 20\n",
    "    hidden_size = 64\n",
    "    embed_size = 50\n",
    "    assert torch.cuda.is_available()\n",
    "\n",
    "    print(\"[!] preparing dataset...\")\n",
    "        \n",
    "    first_lang_train_iter, first_lang_val_iter = load_dataset(batch_size, 'corpus1_10k.txt', val_ratio=0.2)\n",
    "    second_lang_train_iter, second_lang_val_iter = load_dataset(batch_size, 'corpus2_10k.txt', val_ratio=0.2)\n",
    "\n",
    "    # parallel_train_iter1, parallel_train_iter2 = load_parallel_dataset(batch_size)\n",
    "\n",
    "    print(\"[!] Instantiating models...\")\n",
    "\n",
    "    common_encoder = Encoder(en_size, embed_size, hidden_size, copy.deepcopy(en_embeddings), n_layers=2, dropout=0.5)\n",
    "    \n",
    "    # TODO : with None instead of en_embeddings it works\n",
    "\n",
    "    print(en_size)\n",
    "    first_lang_decoder = Decoder(embed_size, hidden_size, en_size, en_embeddings, n_layers=1, dropout=0.5)\n",
    "    second_lang_decoder = Decoder(embed_size, hidden_size, ru_size, en_embeddings, n_layers=1, dropout=0.5)\n",
    "\n",
    "    first_lang_seq2seq = Seq2Seq(common_encoder, first_lang_decoder).cuda()\n",
    "    second_lang_seq2seq = Seq2Seq(common_encoder, second_lang_decoder).cuda()\n",
    "    double_translator = DoubleTranslator(common_encoder, first_lang_decoder, second_lang_decoder).cuda()\n",
    "\n",
    "#     first_lang_optimizer = optim.Adam(first_lang_seq2seq.parameters(), lr=lr)\n",
    "#     second_lang_optimizer = optim.Adam(second_lang_seq2seq.parameters(), lr=lr)\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, double_translator.parameters()), lr=lr)\n",
    "\n",
    "#     print(first_lang_seq2seq)\n",
    "#     print(second_lang_seq2seq)\n",
    "#     print(double_translator)\n",
    "\n",
    "\n",
    "    first_train_data = TrainData(first_lang_seq2seq, first_lang_train_iter, en_size, en_vocab)\n",
    "    second_train_data = TrainData(second_lang_seq2seq, second_lang_train_iter, ru_size, ru_vocab)\n",
    "\n",
    "    first_val_data = TrainData(first_lang_seq2seq, first_lang_val_iter, en_size, en_vocab)\n",
    "    second_val_data = TrainData(second_lang_seq2seq, second_lang_val_iter, ru_size, ru_vocab)\n",
    "    \n",
    "    \n",
    "#     first_train_data = second_train_data,\n",
    "    best_val_loss = None\n",
    "    \n",
    "    \n",
    "    max_shuffle_proc = 0.3\n",
    "    for iter_i in range(1, epochs+1):\n",
    "        curr_proc_shuffle = iter_i * max_shuffle_proc / (epochs+1.0)\n",
    "        print('start train two auto-encoders with swap perc ', curr_proc_shuffle)\n",
    "\n",
    "        train(iter_i, first_train_data, second_train_data, grad_clip, batch_size, optimizer, curr_proc_shuffle)\n",
    "        val_loss = train(iter_i, first_train_data, second_train_data, grad_clip, batch_size, optimizer, curr_proc_shuffle, False)\n",
    "        print(\"[Epoch:%d] val_loss:%5.3f | val_pp:%5.2fS\" % (e, val_loss, math.exp(val_loss)))\n",
    "\n",
    "        if not best_val_loss or val_loss < best_val_loss:\n",
    "            print(\"[!] saving model...\")\n",
    "            if not os.path.isdir(\".save\"):\n",
    "                os.makedirs(\".save\")\n",
    "            torch.save(first_lang_seq2seq.state_dict(), './.save/seq2seq_%d.pt' % (e))\n",
    "            best_val_loss = val_loss\n",
    "    \n",
    "    print('start train auto-encoder + backtracs')\n",
    "    for iter_i in range(1, epochs+1):\n",
    "        train_trans(iter_i, double_translator, optimizer, first_lang_train_iter,\n",
    "                    en_size, grad_clip, en_vocab, batch_size)\n",
    "        \n",
    "    # TODO: use val_iter here\n",
    "#         first_lang_val_loss = evaluate(first_lang_seq2seq, shuffled_train_iter, en_size, en_vocab)\n",
    "#         second_lang_val_loss = evaluate(second_lang_seq2seq, val_iter, en_size, DE, DE)\n",
    "#         double_trans_val_loss = evaluate(double_translator, val_iter, en_size, DE, DE)\n",
    "\n",
    "#         val_loss = first_lang_val_loss + second_lang_val_loss + double_trans_val_loss\n",
    "        \n",
    "#         print(\"[Epoch:%d] val_loss:%5.3f | val_pp:%5.2fS\" % (e, val_loss, math.exp(val_loss)))\n",
    "\n",
    "        # Save the model if the validation loss is the best we've seen so far.\n",
    "#         if not best_val_loss or val_loss < best_val_loss:\n",
    "#             print(\"[!] saving model...\")\n",
    "#             if not os.path.isdir(\".save\"):\n",
    "#                 os.makedirs(\".save\")\n",
    "#             torch.save(first_lang_seq2seq.state_dict(), './.save/seq2seq_%d.pt' % (e))\n",
    "#             best_val_loss = val_loss\n",
    "    test_loss = evaluate(first_lang_seq2seq, test_iter, en_size)\n",
    "    print(\"[TEST] loss:%5.2f\" % test_loss)\n",
    "\n",
    "\n",
    "\n",
    "# main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
