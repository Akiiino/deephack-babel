{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import spacy\n",
    "from torchtext.data import Field, BucketIterator\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext import datasets, data\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import random\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import os\n",
    "import math\n",
    "import argparse\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils import clip_grad_norm\n",
    "from torch.nn import functional as F\n",
    "import numpy as np\n",
    "import copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SOS_TOKEN = '<sos>' # XXX : it must be in vocab!\n",
    "EOS_TOKEN = '<eos>'\n",
    "PAD_INDEX = 13 # TODO\n",
    "UNK_INDEX = 1 # TODO\n",
    "\n",
    "MAX_SENS_LENGTH = 20\n",
    "\n",
    "def load_dataset_old(batch_size, filename):\n",
    "    spacy_en = spacy.load('en')\n",
    "    url = re.compile('(<url>.*</url>)')\n",
    "\n",
    "    def tokenize_en(text):\n",
    "        return [tok.text for tok in spacy_en.tokenizer(url.sub('@URL@', text))]\n",
    "\n",
    "    EN = Field(tokenize=tokenize_en, include_lengths=False, init_token=SOS_TOKEN, eos_token=EOS_TOKEN)\n",
    "    \n",
    "    train = datasets.TranslationDataset('./', exts=(filename, filename), fields=(EN, EN))\n",
    "    EN.build_vocab(train.trg, max_size=10000)\n",
    "    \n",
    "    train_iter, test_iter  = BucketIterator.splits((train, train), batch_size=batch_size, repeat=False)\n",
    "    return train_iter\n",
    "\n",
    "def load_dataset(batch_size, filename):\n",
    "    spacy_en = spacy.load('en')\n",
    "    url = re.compile('(<url>.*</url>)')\n",
    "\n",
    "    def tokenize_en(text):\n",
    "        return [tok.text for tok in spacy_en.tokenizer(url.sub('@URL@', text))]\n",
    "\n",
    "    with open(filename, 'rt') as input_file:\n",
    "        inputs = input_file.readlines()\n",
    "    \n",
    "    inputs = [[SOS_TOKEN] +  tokenize_en(_) + [EOS_TOKEN] for _ in inputs]\n",
    "    inputs = [_ for _ in inputs if len(_) < MAX_SENS_LENGTH]\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embed_size, hidden_size, embeddings = None, n_layers = 1, dropout = 0.5):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        self.embed_size = embed_size\n",
    "        self.embed = nn.Embedding(input_size, embed_size)\n",
    "        if embeddings is not None:\n",
    "            self.embed.weight.data = torch.Tensor(embeddings)#.cuda() # TODO : need cuda here?\n",
    "            \n",
    "        # preparation for freeze\n",
    "        self.embed.weight.requires_grad = False\n",
    "            \n",
    "        self.gru = nn.GRU(embed_size, hidden_size, n_layers, dropout = dropout, bidirectional = True)\n",
    "\n",
    "    def forward(self, src, hidden=None):\n",
    "\n",
    "        embedded = self.embed(src)\n",
    "        outputs, hidden = self.gru(embedded, hidden)\n",
    "        # sum bidirectional outputs\n",
    "        outputs = (outputs[:, :, :self.hidden_size] +\n",
    "                   outputs[:, :, self.hidden_size:])\n",
    "        return outputs, hidden\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
    "        stdv = 1. / math.sqrt(self.v.size(0))\n",
    "        self.v.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        timestep = encoder_outputs.size(0)\n",
    "        h = hidden.repeat(timestep, 1, 1).transpose(0, 1)\n",
    "        encoder_outputs = encoder_outputs.transpose(0, 1)  # [B*T*H]\n",
    "        attn_energies = self.score(h, encoder_outputs)\n",
    "        return F.softmax(attn_energies, dim=1).unsqueeze(1)\n",
    "\n",
    "    def score(self, hidden, encoder_outputs):\n",
    "        # [B*T*2H]->[B*T*H]\n",
    "        energy = self.attn(torch.cat([hidden, encoder_outputs], 2))\n",
    "        energy = energy.transpose(1, 2)  # [B*H*T]\n",
    "        v = self.v.repeat(encoder_outputs.size(0), 1).unsqueeze(1)  # [B*1*H]\n",
    "        energy = torch.bmm(v, energy)  # [B*1*T]\n",
    "        return energy.squeeze(1)  # [B*T]\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, output_size, embeddings = None, n_layers = 1, dropout = 0.2):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embed = nn.Embedding(output_size, embed_size)\n",
    "        \n",
    "        if embeddings is not None:\n",
    "            self.embed.weight.data = torch.Tensor(embeddings)#.cuda() # TODO : need cuda here?\n",
    "            \n",
    "        # preparation for freeze\n",
    "        self.embed.weight.requires_grad = False\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout, inplace=True)\n",
    "        self.attention = Attention(hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size + embed_size, hidden_size, n_layers, dropout=dropout)\n",
    "        self.out = nn.Linear(hidden_size * 2, output_size)\n",
    "\n",
    "    def forward(self, input, last_hidden, encoder_outputs):\n",
    "        # Get the embedding of the current input word (last output word)\n",
    "        embedded = self.embed(input).unsqueeze(0)  # (1,B,N)\n",
    "        embedded = self.dropout(embedded)\n",
    "        # Calculate attention weights and apply to encoder outputs\n",
    "        attn_weights = self.attention(last_hidden[-1], encoder_outputs)\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))  # (B,1,N)\n",
    "        context = context.transpose(0, 1)  # (1,B,N)\n",
    "        # Combine embedded input word and attended context, run through RNN\n",
    "        rnn_input = torch.cat([embedded, context], 2)\n",
    "        output, hidden = self.gru(rnn_input, last_hidden)\n",
    "        output = output.squeeze(0)  # (1,B,N) -> (B,N)\n",
    "        context = context.squeeze(0)\n",
    "        output = self.out(torch.cat([output, context], 1))\n",
    "        output = F.log_softmax(output, dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        batch_size = src.size(1)\n",
    "        max_len = trg.size(0)\n",
    "        vocab_size = self.decoder.output_size\n",
    "        outputs = Variable(torch.zeros(max_len, batch_size, vocab_size)).cuda()\n",
    "\n",
    "        encoder_output, hidden = self.encoder(src)\n",
    "        hidden = hidden[:self.decoder.n_layers]\n",
    "        output = Variable(trg.data[0, :])  # sos\n",
    "        for t in range(1, max_len):\n",
    "            output, hidden, attn_weights = self.decoder(\n",
    "                    output, hidden, encoder_output)\n",
    "            outputs[t] = output\n",
    "            is_teacher = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.data.max(1)[1] # TODO : beam search here\n",
    "            output = Variable(trg.data[t] if is_teacher else top1).cuda()\n",
    "        return outputs\n",
    "\n",
    "    \n",
    "class DoubleTranslator(nn.Module):\n",
    "    def __init__(self, common_encoder, first_lang_decoder, second_lang_decoder):\n",
    "        super(DoubleTranslator, self).__init__()\n",
    "        self.common_encoder = common_encoder\n",
    "        self.first_lang_decoder = first_lang_decoder\n",
    "        self.second_lang_decoder = second_lang_decoder\n",
    "        self.is_from_first_lang_to_second = True\n",
    "        \n",
    "    def set_is_from_first_lang_to_second(self, value):\n",
    "        self.is_from_first_lang_to_second = value\n",
    "\n",
    "    def get_is_from_first_lang_to_second(self):\n",
    "        return self.is_from_first_lang_to_second\n",
    "    \n",
    "    def forward_one_lang(self, src, trg, teacher_forcing_ratio=0.5, is_first_lang = True):\n",
    "        batch_size = src.size(1)\n",
    "        max_len = trg.size(0)\n",
    "\n",
    "        # if is_first_lang: en_word -> encoder -> ru_decoder -> encoder -> en_decoder -> en_word\n",
    "        decoder = self.first_lang_decoder if not is_first_lang else self.second_lang_decoder\n",
    "        \n",
    "        vocab_size = decoder.output_size\n",
    "        outputs = Variable(torch.zeros(max_len, batch_size, vocab_size)).cuda()\n",
    "        \n",
    "        encoder_output, hidden = self.common_encoder(src)\n",
    "        hidden = hidden[:decoder.n_layers]\n",
    "        output = Variable(trg.data[0, :])  # sos\n",
    "        for t in range(1, max_len):\n",
    "            output, hidden, attn_weights = decoder(output, hidden, encoder_output)\n",
    "            outputs[t] = output\n",
    "            is_teacher = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.data.max(1)[1] # TODO : beam search here\n",
    "            output = Variable(trg.data[t] if is_teacher else top1).cuda()\n",
    "            \n",
    "        return outputs\n",
    "    \n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        output_first_lang = self.forward_one_lang(src, trg, teacher_forcing_ratio, self.get_is_from_first_lang_to_second())\n",
    "        return self.forward_one_lang(src, trg, teacher_forcing_ratio, not self.get_is_from_first_lang_to_second())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "[ru_vocab]:44323 [en_vocab]:41976 [common_size]:41976\n",
      "[!] preparing dataset...\n",
      "[!] Instantiating models...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "cuda runtime error (59) : device-side assert triggered at /pytorch/torch/lib/THC/THCTensorCopy.cu:204",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-1b609a27ef06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0mbest_val_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst_train_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msecond_train_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_clip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m \u001b[0;31m#         train(e, second_lang_seq2seq, second_lang_optimizer, second_lang_train_iter, ru_size, grad_clip, ru_vocab, batch_size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-1b609a27ef06>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(e, train_data1, train_data2, grad_clip, batch_size, optimizer)\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mget_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msents2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_data2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-1b609a27ef06>\u001b[0m in \u001b[0;36mget_loss\u001b[0;34m(sents, train_data1)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0moutput1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m         loss1 = F.cross_entropy(output1[1:].view(-1, train_data1.vocab_size),\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-0010c444518b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, trg, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_len\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m             output, hidden, attn_weights = self.decoder(\n\u001b[0;32m--> 108\u001b[0;31m                     output, hidden, encoder_output)\n\u001b[0m\u001b[1;32m    109\u001b[0m             \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m             \u001b[0mis_teacher\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mteacher_forcing_ratio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-0010c444518b>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, last_hidden, encoder_outputs)\u001b[0m\n\u001b[1;32m     84\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# (1,B,N) -> (B,N)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m    833\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    834\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 835\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    836\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    837\u001b[0m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cuda runtime error (59) : device-side assert triggered at /pytorch/torch/lib/THC/THCTensorCopy.cu:204"
     ]
    }
   ],
   "source": [
    "def batch(iterable, batch_size = 1):\n",
    "    all_length = len(iterable)\n",
    "    for ndx in range(0, all_length, batch_size):\n",
    "        yield iterable[ndx:min(ndx + batch_size, all_length)]\n",
    "\n",
    "def get_vocab(emb_plain):\n",
    "    result = {}\n",
    "    for i, line in enumerate(emb_plain):\n",
    "        word, vector = line.split(' ', 1)\n",
    "        result[word] = len(result)  \n",
    "\n",
    "    return result\n",
    "\n",
    "def get_embeddings(emb_plain, emb_size):\n",
    "    result = np.ndarray((len(emb_plain), emb_size), dtype='float32')\n",
    "    for i, line in enumerate(emb_plain):\n",
    "        word, vector = line.split(' ', 1)\n",
    "        result[i] = vector.split()  \n",
    "    return result\n",
    "        \n",
    "def evaluate(model, val_iter, vocab_size, vocab):\n",
    "    model.eval()\n",
    "    pad = PAD_INDEX\n",
    "    total_loss = 0\n",
    "    for b, batch in enumerate(val_iter):\n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "        src = Variable(src.data.cuda(), volatile=True)\n",
    "        trg = Variable(trg.data.cuda(), volatile=True)\n",
    "        output = model(src, trg)\n",
    "        loss = F.cross_entropy(output[1:].view(-1, vocab_size),\n",
    "                               trg[1:].contiguous().view(-1),\n",
    "                               ignore_index=pad)\n",
    "        total_loss += loss.data[0]\n",
    "    return total_loss / len(val_iter)\n",
    "\n",
    "class TrainData:\n",
    "    def __init__(self, model, train_iter, vocab_size, vocab):\n",
    "        self.model = model\n",
    "        self.train_iter = train_iter\n",
    "        self.vocab_size = vocab_size\n",
    "        self.vocab = vocab\n",
    "\n",
    "def train(e, train_data1, train_data2, grad_clip, batch_size, optimizer):\n",
    "    train_data1.model.train()\n",
    "    train_data2.model.train()\n",
    "    \n",
    "    total_loss = 0\n",
    "    pad = PAD_INDEX\n",
    "    \n",
    "    common_length = min(len(train_data1.train_iter), len(train_data2.train_iter))\n",
    "    common_data = list(zip(train_data1.train_iter[:common_length], train_data2.train_iter[:common_length]))\n",
    "    \n",
    "    def get_loss(sents, train_data1):\n",
    "        max_length_in_batch1 = max([len(s) for s in sents1])\n",
    "        \n",
    "        t1 = np.array([sents2inds(sent, max_length_in_batch1, train_data1.vocab) for sent in sents1])\n",
    "        src1 = Variable(torch.from_numpy(t1.T))\n",
    "        trg1 = Variable(torch.from_numpy(t1.T))\n",
    "        src1, trg1 = src1.cuda(), trg1.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output1 = train_data1.model(src1, trg1)\n",
    "\n",
    "        loss1 = F.cross_entropy(output1[1:].view(-1, train_data1.vocab_size),\n",
    "                               trg1[1:].contiguous().view(-1),\n",
    "                               ignore_index=pad)\n",
    "        return loss1\n",
    "    \n",
    "    for b, sents in enumerate(batch(common_data, batch_size=batch_size)):\n",
    "        sents1 = [s[0] for s in sents]\n",
    "        sents2 = [s[1] for s in sents]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = get_loss(sents1, train_data1) + get_loss(sents2, train_data2) \n",
    "        loss.backward()\n",
    "        \n",
    "        clip_grad_norm(train_data1.model.parameters(), grad_clip)\n",
    "        clip_grad_norm(train_data2.model.parameters(), grad_clip)\n",
    "        \n",
    "        optimizer.step()\n",
    "        total_loss += loss.data[0]\n",
    "\n",
    "        if b % 100 == 0 and b != 0:\n",
    "            total_loss = total_loss / 100\n",
    "            print(\"[%d][loss:%5.2f][pp:%5.2f]\" %\n",
    "                  (b, total_loss, math.exp(total_loss)))\n",
    "            total_loss = 0\n",
    "\n",
    "\n",
    "def train_trans(e, model, optimizer, train_iter,\n",
    "                    vocab_size, grad_clip, vocab, batch_size):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    pad = PAD_INDEX\n",
    "    \n",
    "    for b, sents in enumerate(batch(train_iter, batch_size=batch_size)):\n",
    "        max_length_in_batch = max([len(s) for s in sents])\n",
    "        \n",
    "        t = np.array([sents2inds(sent, max_length_in_batch, vocab) for sent in sents])\n",
    "        src = Variable(torch.from_numpy(t.T))\n",
    "        trg = Variable(torch.from_numpy(t.T))\n",
    "        \n",
    "        src, trg = src.cuda(), trg.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg)\n",
    "        loss = F.cross_entropy(output[1:].view(-1, vocab_size),\n",
    "                               trg[1:].contiguous().view(-1),\n",
    "                               ignore_index=pad)\n",
    "        loss.backward()\n",
    "        clip_grad_norm(model.parameters(), grad_clip)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.data[0]\n",
    "\n",
    "        if b % 100 == 0 and b != 0:\n",
    "            total_loss = total_loss / 100\n",
    "            print(\"[%d][loss:%5.2f][pp:%5.2f]\" %\n",
    "                  (b, total_loss, math.exp(total_loss)))\n",
    "            total_loss = 0\n",
    "\n",
    "            \n",
    "def create_ind2word(vocab):\n",
    "    return {item[1]:item[0] for item in vocab.items()}\n",
    "            \n",
    "def create_emb_and_dict_by_emb_and_dict(vocab, embeddings, ind2word, dict_plain):\n",
    "    result_emb = []\n",
    "    result_dict = {}\n",
    "    \n",
    "    result_emb2 = []\n",
    "    result_dict2 = {}\n",
    "    \n",
    "    res_embedding_inds = []\n",
    "    \n",
    "    # XXX : first 3 items just copy : SOS EOS ...\n",
    "#     count = 3\n",
    "#     for i in range(count):\n",
    "#         en_word = ind2word.get(i, -1)\n",
    "#         if en_word == -1:\n",
    "#             continue\n",
    "            \n",
    "#         result_emb.append(embeddings[i])\n",
    "#         result_dict[en_word] = len(result_dict)\n",
    "        \n",
    "#         result_emb2.append(embeddings[i])\n",
    "#         result_dict2[en_word] = len(result_dict)\n",
    "    \n",
    "    for word_pair in dict_plain:\n",
    "        en_word, ru_word = word_pair.split()\n",
    "        \n",
    "        if en_word not in vocab or ru_word in result_emb:\n",
    "            continue\n",
    "        \n",
    "        if en_word in result_dict2:\n",
    "            result_dict[ru_word] = result_dict2[en_word]\n",
    "            continue\n",
    "        \n",
    "        res_embedding_inds.append(vocab[en_word])\n",
    "        result_dict[ru_word] = len(result_dict2)\n",
    "        result_dict2[en_word] = len(result_dict2)\n",
    "        \n",
    "    result_emb = embeddings[res_embedding_inds]\n",
    "    return result_dict, result_emb, result_dict2, result_emb\n",
    "\n",
    "def sents2inds(sents, max_length, vocab):\n",
    "    pad_ind = PAD_INDEX\n",
    "    return np.pad(np.array([vocab.get(i, UNK_INDEX) for i in sents]),\n",
    "                (0, max_length - len(sents)),  mode='constant', constant_values=(pad_ind))\n",
    "    \n",
    "# def main():\n",
    "if True:    \n",
    "    with open('glove/glove.6B.50d.txt', 'rt') as emb_file:\n",
    "        en_emb_plain = emb_file.readlines()\n",
    "\n",
    "    with open('en-ru.txt', 'rt') as dict_file:\n",
    "        dict_en_ru_plain = dict_file.readlines()\n",
    "        \n",
    "    en_vocab = get_vocab(en_emb_plain)\n",
    "    en_embeddings = get_embeddings(en_emb_plain, 50)\n",
    "    en_ind2word = create_ind2word(en_vocab)\n",
    "    \n",
    "    ru_vocab, ru_embeddings, en_vocab, en_embeddings = create_emb_and_dict_by_emb_and_dict(en_vocab, en_embeddings, en_ind2word, dict_en_ru_plain)\n",
    "#     ru_embeddings = en_embeddings = None\n",
    "\n",
    "    en_size = len(en_vocab)\n",
    "    ru_size = len(ru_vocab)\n",
    "    \n",
    "    common_size = min(en_size, ru_size)\n",
    "    \n",
    "    print(\"[ru_vocab]:%d [en_vocab]:%d [common_size]:%d\" % (ru_size, en_size, common_size))\n",
    "\n",
    "    en_size = common_size\n",
    "    ru_size = common_size\n",
    "\n",
    "    \n",
    "    epochs = 10\n",
    "    grad_clip = 10.0\n",
    "    lr = 0.0001\n",
    "    en_size = len(en_emb_plain)\n",
    "    \n",
    "    batch_size = 5\n",
    "    hidden_size = 64\n",
    "    embed_size = 50\n",
    "    assert torch.cuda.is_available()\n",
    "\n",
    "    print(\"[!] preparing dataset...\")\n",
    "    \n",
    "    first_lang_train_iter = load_dataset(batch_size, 'corpus1_10k.txt')\n",
    "    second_lang_train_iter = load_dataset(batch_size, 'corpus2_10k.txt')\n",
    "            \n",
    "\n",
    "    print(\"[!] Instantiating models...\")\n",
    "\n",
    "    common_encoder = Encoder(en_size, embed_size, hidden_size, copy.deepcopy(en_embeddings), n_layers=2, dropout=0.5)\n",
    "    \n",
    "    # TODO : with None instead of en_embeddings it works\n",
    "    first_lang_decoder = Decoder(embed_size, hidden_size, en_size, en_embeddings, n_layers=1, dropout=0.5)\n",
    "    second_lang_decoder = Decoder(embed_size, hidden_size, ru_size, en_embeddings, n_layers=1, dropout=0.5)\n",
    "\n",
    "    first_lang_seq2seq = Seq2Seq(common_encoder, first_lang_decoder).cuda()\n",
    "    second_lang_seq2seq = Seq2Seq(common_encoder, second_lang_decoder).cuda()\n",
    "    double_translator = DoubleTranslator(common_encoder, first_lang_decoder, second_lang_decoder).cuda()\n",
    "\n",
    "#     first_lang_optimizer = optim.Adam(first_lang_seq2seq.parameters(), lr=lr)\n",
    "#     second_lang_optimizer = optim.Adam(second_lang_seq2seq.parameters(), lr=lr)\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, double_translator.parameters()), lr=lr)\n",
    "\n",
    "#     print(first_lang_seq2seq)\n",
    "#     print(second_lang_seq2seq)\n",
    "#     print(double_translator)\n",
    "\n",
    "\n",
    "    first_train_data = TrainData(first_lang_seq2seq, first_lang_train_iter, en_size, en_vocab)\n",
    "    second_train_data = TrainData(second_lang_seq2seq, second_lang_train_iter, ru_size, ru_vocab)\n",
    "\n",
    "#     first_train_data = second_train_data,\n",
    "    best_val_loss = None\n",
    "    for e in range(1, epochs+1):\n",
    "        train(e, first_train_data, second_train_data, grad_clip, batch_size, optimizer)\n",
    "\n",
    "#         train(e, second_lang_seq2seq, second_lang_optimizer, second_lang_train_iter, ru_size, grad_clip, ru_vocab, batch_size)\n",
    "#         train_trans(e, double_translator, decoder_optimizer, first_lang_train_iter,\n",
    "#                     en_size, grad_clip, en_vocab, batch_size)\n",
    "        \n",
    "    # TODO: use val_iter here\n",
    "#         first_lang_val_loss = evaluate(first_lang_seq2seq, shuffled_train_iter, en_size, en_vocab)\n",
    "#         second_lang_val_loss = evaluate(second_lang_seq2seq, val_iter, en_size, DE, DE)\n",
    "#         double_trans_val_loss = evaluate(double_translator, val_iter, en_size, DE, DE)\n",
    "\n",
    "#         val_loss = first_lang_val_loss + second_lang_val_loss + double_trans_val_loss\n",
    "        \n",
    "        print(\"[Epoch:%d] val_loss:%5.3f | val_pp:%5.2fS\" % (e, val_loss, math.exp(val_loss)))\n",
    "\n",
    "        # Save the model if the validation loss is the best we've seen so far.\n",
    "        if not best_val_loss or val_loss < best_val_loss:\n",
    "            print(\"[!] saving model...\")\n",
    "            if not os.path.isdir(\".save\"):\n",
    "                os.makedirs(\".save\")\n",
    "            torch.save(first_lang_seq2seq.state_dict(), './.save/seq2seq_%d.pt' % (e))\n",
    "            best_val_loss = val_loss\n",
    "    test_loss = evaluate(first_lang_seq2seq, test_iter, en_size)\n",
    "    print(\"[TEST] loss:%5.2f\" % test_loss)\n",
    "\n",
    "\n",
    "\n",
    "# main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
