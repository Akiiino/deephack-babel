{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import random\n",
    "from torch import nn\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, embed_size, hidden_size, embeddings,\n",
    "                 n_layers=1, dropout=0.5):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.embed_size = embed_size\n",
    "        self.embed = nn.Embedding(input_size, embed_size)\n",
    "        \n",
    "        if embeddings is not None:\n",
    "            self.embed.weight.data = torch.Tensor(embeddings)#.cuda()\n",
    "#         self.embedding.weight.requires_grad = False\n",
    "            \n",
    "        self.gru = nn.GRU(embed_size, hidden_size, n_layers,\n",
    "                          dropout=dropout, bidirectional=True)\n",
    "\n",
    "    def forward(self, src, hidden=None):\n",
    "        embedded = self.embed(src)\n",
    "        outputs, hidden = self.gru(embedded, hidden)\n",
    "        # sum bidirectional outputs\n",
    "        outputs = (outputs[:, :, :self.hidden_size] +\n",
    "                   outputs[:, :, self.hidden_size:])\n",
    "        return outputs, hidden\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, hidden_size)\n",
    "        self.v = nn.Parameter(torch.rand(hidden_size))\n",
    "        stdv = 1. / math.sqrt(self.v.size(0))\n",
    "        self.v.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, hidden, encoder_outputs):\n",
    "        timestep = encoder_outputs.size(0)\n",
    "        h = hidden.repeat(timestep, 1, 1).transpose(0, 1)\n",
    "        encoder_outputs = encoder_outputs.transpose(0, 1)  # [B*T*H]\n",
    "        attn_energies = self.score(h, encoder_outputs)\n",
    "        return F.softmax(attn_energies, dim=1).unsqueeze(1)\n",
    "\n",
    "    def score(self, hidden, encoder_outputs):\n",
    "        # [B*T*2H]->[B*T*H]\n",
    "        energy = self.attn(torch.cat([hidden, encoder_outputs], 2))\n",
    "        energy = energy.transpose(1, 2)  # [B*H*T]\n",
    "        v = self.v.repeat(encoder_outputs.size(0), 1).unsqueeze(1)  # [B*1*H]\n",
    "        energy = torch.bmm(v, energy)  # [B*1*T]\n",
    "        return energy.squeeze(1)  # [B*T]\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embed_size, hidden_size, output_size, embeddings,\n",
    "                 n_layers=1, dropout=0.2):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embed_size = embed_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embed = nn.Embedding(output_size, embed_size)\n",
    "        \n",
    "        if embeddings is not None:\n",
    "            self.embed.weight.data = torch.Tensor(embeddings)#.cuda()\n",
    "#         self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout, inplace=True)\n",
    "        self.attention = Attention(hidden_size)\n",
    "        self.gru = nn.GRU(hidden_size + embed_size, hidden_size,\n",
    "                          n_layers, dropout=dropout)\n",
    "        self.out = nn.Linear(hidden_size * 2, output_size)\n",
    "\n",
    "    def forward(self, input, last_hidden, encoder_outputs):\n",
    "        # Get the embedding of the current input word (last output word)\n",
    "        embedded = self.embed(input).unsqueeze(0)  # (1,B,N)\n",
    "        embedded = self.dropout(embedded)\n",
    "        # Calculate attention weights and apply to encoder outputs\n",
    "        attn_weights = self.attention(last_hidden[-1], encoder_outputs)\n",
    "        context = attn_weights.bmm(encoder_outputs.transpose(0, 1))  # (B,1,N)\n",
    "        context = context.transpose(0, 1)  # (1,B,N)\n",
    "        # Combine embedded input word and attended context, run through RNN\n",
    "        rnn_input = torch.cat([embedded, context], 2)\n",
    "        output, hidden = self.gru(rnn_input, last_hidden)\n",
    "        output = output.squeeze(0)  # (1,B,N) -> (B,N)\n",
    "        context = context.squeeze(0)\n",
    "        output = self.out(torch.cat([output, context], 1))\n",
    "        output = F.log_softmax(output, dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        print(src)\n",
    "        batch_size = src.size(1)\n",
    "        max_len = trg.size(0)\n",
    "        vocab_size = self.decoder.output_size\n",
    "        outputs = Variable(torch.zeros(max_len, batch_size, vocab_size)).cuda()\n",
    "\n",
    "        encoder_output, hidden = self.encoder(src)\n",
    "        hidden = hidden[:self.decoder.n_layers]\n",
    "        output = Variable(trg.data[0, :])  # sos\n",
    "        for t in range(1, max_len):\n",
    "            output, hidden, attn_weights = self.decoder(\n",
    "                    output, hidden, encoder_output)\n",
    "            outputs[t] = output\n",
    "            is_teacher = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.data.max(1)[1]\n",
    "            output = Variable(trg.data[t] if is_teacher else top1).cuda()\n",
    "        return outputs\n",
    "\n",
    "    \n",
    "class DoubleTranslator(nn.Module):\n",
    "    def __init__(self, common_encoder, first_lang_decoder, second_lang_decoder):\n",
    "        super(DoubleTranslator, self).__init__()\n",
    "        self.common_encoder = common_encoder\n",
    "        self.first_lang_decoder = first_lang_decoder\n",
    "        self.second_lang_decoder = second_lang_decoder\n",
    "        \n",
    "        self.is_from_first_lang_to_second = True\n",
    "        \n",
    "    def set_is_from_first_lang_to_second(self, value):\n",
    "        self.is_from_first_lang_to_second = value\n",
    "\n",
    "    def forward_one_lang(self, src, trg, teacher_forcing_ratio=0.5, is_first_lang = True):\n",
    "        batch_size = src.size(1)\n",
    "        max_len = trg.size(0)\n",
    "        vocab_size = self.decoder.output_size\n",
    "        outputs = Variable(torch.zeros(max_len, batch_size, vocab_size)).cuda()\n",
    "\n",
    "        decoder = self.first_lang_decoder if is_first_lang else self.second_lang_decoder\n",
    "        \n",
    "        encoder_output, hidden = self.common_encoder(src)\n",
    "        hidden = hidden[:self.decoder.n_layers]\n",
    "        output = Variable(trg.data[0, :])  # sos\n",
    "        for t in range(1, max_len):\n",
    "            output, hidden, attn_weights = self.decoder(output, hidden, encoder_output)\n",
    "            outputs[t] = output\n",
    "            is_teacher = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.data.max(1)[1]\n",
    "            output = Variable(trg.data[t] if is_teacher else top1).cuda()\n",
    "            \n",
    "        return outputs\n",
    "    \n",
    "    \n",
    "    \n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        output_first_lang = self.forward_one_lang(src, trg, teacher_forcing_ratio, is_from_first_lang_to_second)\n",
    "        return self.forward_one_lang(src, trg, teacher_forcing_ratio, not is_from_first_lang_to_second)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[!] preparing dataset...\n",
      "[TRAIN]:1000\n",
      "[!] Instantiating models...\n",
      "batch.src\n",
      "[    1     1  2582 74426     1]\n",
      "\n",
      "     1      1   2582  74426      1\n",
      "[torch.cuda.LongTensor of size 1x5 (GPU 0)]\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "save_for_backward can only save input or output tensors, but argument 0 doesn't satisfy this condition",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-d1286f0ef372>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 219\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2-d1286f0ef372>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    193\u001b[0m     \u001b[0mbest_val_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 195\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst_lang_seq2seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfirst_lang_optimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffled_train_iter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0men_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_clip\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    196\u001b[0m \u001b[0;31m#         train(e, second_lang_seq2seq, second_lang_optimizer, shuffled_train_iter, en_size, grad_clip, DE, DE)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    197\u001b[0m \u001b[0;31m#         train(e, double_translator, decoder_optimizer, shuffled_train_iter, en_size, grad_clip, DE, DE)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-d1286f0ef372>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(e, model, optimizer, train_iter, vocab_size, grad_clip, first_lang_field, second_lang_field)\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    130\u001b[0m         loss = F.cross_entropy(output[1:].view(-1, vocab_size),\n\u001b[1;32m    131\u001b[0m                                \u001b[0mtrg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-dbe5ab3456c2>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, trg, teacher_forcing_ratio)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0mencoder_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_layers\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# sos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-dbe5ab3456c2>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, hidden)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0membedded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgru\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0;31m# sum bidirectional outputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_pre_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    326\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m         )\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: save_for_backward can only save input or output tensors, but argument 0 doesn't satisfy this condition"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import argparse\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils import clip_grad_norm\n",
    "from torch.nn import functional as F\n",
    "# import model #import *#Encoder, Decoder, Seq2Seq, DoubleTranslator\n",
    "from torchtext import datasets, data\n",
    "\n",
    "from torchtext.data import Field, BucketIterator\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "with open('glove/glove.6B.50d.txt', 'rt') as emb_file:\n",
    "    en_emb_plain = emb_file.readlines()\n",
    "\n",
    "    \n",
    "def get_vocab(emb_plain):\n",
    "    result = {}\n",
    "    for i, line in enumerate(en_emb_plain):\n",
    "#         if i > 4:\n",
    "#             break\n",
    "        word, vector = line.split(' ', 1)\n",
    "        result[word] = len(result)  \n",
    "    \n",
    "    return result\n",
    "\n",
    "en_vocab = get_vocab(en_emb_plain)\n",
    "\n",
    "def get_embeddings(emb_plain, emb_size):\n",
    "    result = np.ndarray((len(emb_plain), emb_size), dtype='float32')\n",
    "    for i, line in enumerate(en_emb_plain):\n",
    "#         if i > 4:\n",
    "#             break\n",
    "        word, vector = line.split(' ', 1)\n",
    "        result[i] = vector.split()  \n",
    "    return result\n",
    "\n",
    "    \n",
    "#     max_rank = max(lex.rank for lex in vocab if lex.has_vector)\n",
    "#     vectors = np.ndarray((max_rank+1, vocab.vectors_length), dtype='float32')\n",
    "#     for lex in vocab:\n",
    "#         if lex.has_vector:\n",
    "#             vectors[lex.rank] = lex.vector\n",
    "#     return vectors\n",
    "\n",
    "get_embeddings(en_emb_plain, 50)\n",
    "    \n",
    "\n",
    "def ugly_swap(array, vocab, percent_of_swaps = 0.5):\n",
    "    assert percent_of_swaps < 1\n",
    "    result = array.copy()\n",
    "    count_of_swaps = (int)(percent_of_swaps * len(array))\n",
    "    \n",
    "    indeces = np.random.randint(array.shape[0], size=(count_of_swaps, 2))\n",
    "    \n",
    "    result = np.array([vocab.get(word, 1) for word in array])\n",
    "    \n",
    "    for index_pair in indeces:\n",
    "        result[index_pair[0]], result[index_pair[1]] = result[index_pair[1]], result[index_pair[0]]\n",
    "    return result\n",
    "\n",
    "# ugly_swap(np.array([1,2,3,4, 5,6]), 0)\n",
    "\n",
    "def load_mono_dataset(filename):\n",
    "    EN = Field(include_lengths=True, init_token='<sos>', eos_token='<eos>',\n",
    "               use_vocab = False, preprocessing=lambda sent : ugly_swap(np.array(sent), en_vocab, percent_of_swaps=0))\n",
    "\n",
    "    train = datasets.TranslationDataset('./', exts=(filename, filename), fields=(EN, EN))\n",
    "\n",
    "    # DE.build_vocab(train.src, min_freq=2)\n",
    "    EN.build_vocab(train.scr, max_size=10000)\n",
    "\n",
    "    return train.examples, EN\n",
    "\n",
    "def parse_arguments():\n",
    "    p = argparse.ArgumentParser(description='Hyperparams')\n",
    "    p.add_argument('-epochs', type=int, default=100,\n",
    "                   help='number of epochs for train')\n",
    "    p.add_argument('-batch_size', type=int, default=32,\n",
    "                   help='number of epochs for train')\n",
    "    p.add_argument('-lr', type=float, default=0.0001,\n",
    "                   help='initial learning rate')\n",
    "    p.add_argument('-grad_clip', type=float, default=10.0,\n",
    "                   help='initial learning rate')\n",
    "    return p.parse_args()\n",
    "\n",
    "\n",
    "def evaluate(model, val_iter, vocab_size, first_lang_field, second_lang_field):\n",
    "    model.eval()\n",
    "    pad = second_lang_field.vocab.stoi['<pad>']\n",
    "    total_loss = 0\n",
    "    for b, batch in enumerate(val_iter):\n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "        src = Variable(src.data.cuda(), volatile=True)\n",
    "        trg = Variable(trg.data.cuda(), volatile=True)\n",
    "        output = model(src, trg)\n",
    "        loss = F.cross_entropy(output[1:].view(-1, vocab_size),\n",
    "                               trg[1:].contiguous().view(-1),\n",
    "                               ignore_index=pad)\n",
    "        total_loss += loss.data[0]\n",
    "    return total_loss / len(val_iter)\n",
    "\n",
    "\n",
    "def train(e, model, optimizer, train_iter, vocab_size, grad_clip, first_lang_field, second_lang_field):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    pad = second_lang_field.vocab.stoi['<pad>']\n",
    "    for b, batch in enumerate(train_iter):\n",
    "        src = batch.src\n",
    "\n",
    "        print('batch.src')\n",
    "        print(batch.src)\n",
    "\n",
    "        src = torch.from_numpy(src)\n",
    "\n",
    "        trg = batch.trg\n",
    "\n",
    "        trg = torch.from_numpy(batch.trg)\n",
    "        \n",
    "#         src = Variable(src.cuda(), volatile=True)\n",
    "#         trg = Variable(trg.cuda(), volatile=True)\n",
    "        \n",
    "        src, trg = src.view(1,-1).cuda(), trg.view(1,-1).cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg)\n",
    "        loss = F.cross_entropy(output[1:].view(-1, vocab_size),\n",
    "                               trg[1:].contiguous().view(-1),\n",
    "                               ignore_index=pad)\n",
    "        loss.backward()\n",
    "        clip_grad_norm(model.parameters(), grad_clip)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.data[0]\n",
    "\n",
    "        if b % 100 == 0 and b != 0:\n",
    "            total_loss = total_loss / 100\n",
    "            print(\"[%d][loss:%5.2f][pp:%5.2f]\" %\n",
    "                  (b, total_loss, math.exp(total_loss)))\n",
    "            total_loss = 0\n",
    "\n",
    "\n",
    "def main():\n",
    "#     args = parse_arguments()\n",
    "\n",
    "    epochs = 100\n",
    "    batch_size = 1\n",
    "    lr = 0.0001\n",
    "    grad_clip = 10.0\n",
    "    \n",
    "    hidden_size = 512\n",
    "    embed_size = 256\n",
    "    assert torch.cuda.is_available()\n",
    "\n",
    "    print(\"[!] preparing dataset...\")\n",
    "    \n",
    "    shuffled_train_iter, EN = load_mono_dataset('corpus1_cutted.txt')\n",
    "#     train_iter, val_iter, test_iter, DE, EN = load_dataset(batch_size)\n",
    "#     shuffled_train_iter, shuffled_val_iter, shuffled_test_iter, DE, EN = load_dataset(batch_size)\n",
    "    \n",
    "#     de_size, en_size = len(DE.vocab), len(EN.vocab)\n",
    "    \n",
    "    print(\"[TRAIN]:%d\" % len(shuffled_train_iter))\n",
    "    \n",
    "#     print(\"[TRAIN]:%d (dataset:%d)\\t[TEST]:%d (dataset:%d)\"\n",
    "#           % (len(train_iter), len(train_iter.dataset),\n",
    "#              len(test_iter), len(test_iter.dataset)))\n",
    "#     print(\"[DE_vocab]:%d [en_vocab]:%d\" % (de_size, en_size))\n",
    "\n",
    "    print(\"[!] Instantiating models...\")\n",
    "    \n",
    "    en_size = len(en_emb_plain)\n",
    "    \n",
    "    embeddings = get_embeddings(en_emb_plain, 50)\n",
    "    common_encoder = Encoder(en_size, embed_size, hidden_size, embeddings, n_layers=2, dropout=0.5)#, embeddings = embeddings)\n",
    "    first_lang_decoder = Decoder(embed_size, hidden_size, en_size, embeddings, n_layers=1, dropout=0.5)#, embeddings = embeddings)\n",
    "#     second_lang_decoder = Decoder(embed_size, hidden_size, en_size, n_layers=1, dropout=0.5)\n",
    "\n",
    "    first_lang_seq2seq = Seq2Seq(common_encoder, first_lang_decoder).cuda()\n",
    "#     second_lang_seq2seq = Seq2Seq(common_encoder, second_lang_decoder).cuda()\n",
    "#     double_translator = DoubleTranslator(common_encoder, first_lang_decoder, second_lang_decoder).cuda()\n",
    "\n",
    "    first_lang_optimizer = optim.Adam(first_lang_seq2seq.parameters(), lr=lr)\n",
    "#     second_lang_optimizer = optim.Adam(second_lang_seq2seq.parameters(), lr=lr)\n",
    "#     decoder_optimizer = optim.Adam(double_translator.parameters(), lr=lr)\n",
    "\n",
    "#     print(first_lang_seq2seq)\n",
    "#     print(second_lang_seq2seq)\n",
    "#     print(double_translator)\n",
    "\n",
    "    best_val_loss = None\n",
    "    for e in range(1, epochs+1):\n",
    "        train(e, first_lang_seq2seq, first_lang_optimizer, shuffled_train_iter, en_size, grad_clip, EN, EN)\n",
    "#         train(e, second_lang_seq2seq, second_lang_optimizer, shuffled_train_iter, en_size, grad_clip, DE, DE)\n",
    "#         train(e, double_translator, decoder_optimizer, shuffled_train_iter, en_size, grad_clip, DE, DE)\n",
    "        \n",
    "    # TODO: use val_iter here\n",
    "        first_lang_val_loss = evaluate(first_lang_seq2seq, shuffled_train_iter, en_size, EN, EN)\n",
    "#         second_lang_val_loss = evaluate(second_lang_seq2seq, val_iter, en_size, DE, DE)\n",
    "#         double_trans_val_loss = evaluate(double_translator, val_iter, en_size, DE, DE)\n",
    "\n",
    "        val_loss = first_lang_val_loss # + second_lang_val_loss + double_trans_val_loss\n",
    "        \n",
    "        print(\"[Epoch:%d] val_loss:%5.3f | val_pp:%5.2fS\" % (e, val_loss, math.exp(val_loss)))\n",
    "\n",
    "        # Save the model if the validation loss is the best we've seen so far.\n",
    "        if not best_val_loss or val_loss < best_val_loss:\n",
    "            print(\"[!] saving model...\")\n",
    "            if not os.path.isdir(\".save\"):\n",
    "                os.makedirs(\".save\")\n",
    "            torch.save(first_lang_seq2seq.state_dict(), './.save/seq2seq_%d.pt' % (e))\n",
    "            best_val_loss = val_loss\n",
    "    test_loss = evaluate(first_lang_seq2seq, test_iter, en_size, DE, EN)\n",
    "    print(\"[TEST] loss:%5.2f\" % test_loss)\n",
    "\n",
    "\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
